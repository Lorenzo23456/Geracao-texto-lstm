Claro! Aqui estÃ¡ uma versÃ£o mais bonita e organizada do seu relatÃ³rio em Markdown, com melhor estrutura visual, uso de emojis para destacar seÃ§Ãµes e tabelas para facilitar a leitura:

---

# ğŸ§  GeraÃ§Ã£o de Textos com Redes Neurais Recorrentes  
## RelatÃ³rio de ImplementaÃ§Ã£o e AvaliaÃ§Ã£o de Modelo LSTM

### ğŸ‘¥ Autores e AfiliaÃ§Ã£o

- **Autores**: Arthur Lima de Menezes, JoÃ£o Pedro Huppes Arenales, Lorenzo de Castro, PÃ¢mela da Silva Paes  
- **AfiliaÃ§Ã£o**: Faculdade de ComputaÃ§Ã£o â€” Universidade Federal de Mato Grosso do Sul (UFMS)

---

### ğŸ“Œ Resumo do Projeto

Este projeto abordou a implementaÃ§Ã£o e avaliaÃ§Ã£o de um modelo de linguagem baseado em LSTM (Long Short-Term Memory) para geraÃ§Ã£o de texto em portuguÃªs. O processo envolveu desde a construÃ§Ã£o do corpus atÃ© a anÃ¡lise crÃ­tica dos resultados.

- **Corpus**: 441 letras de Zeca Pagodinho  
- **Embeddings**: Aprendidos do zero  
- **MotivaÃ§Ã£o**: VocabulÃ¡rio rico e estilo cultural marcante do artista

---

### ğŸ“Š Resultados Principais

- **ğŸ”º Alta Perplexidade**: PPL â‰ˆ 654.4384 no conjunto de teste  
- **âš ï¸ Overfitting**: Perda no treino caiu, enquanto a de validaÃ§Ã£o aumentou  
- **ğŸ“ Qualidade dos Textos**: Baixa coerÃªncia e alta repetiÃ§Ã£o

---

### ğŸ§ª Materiais e MÃ©todos

#### ğŸ¼ Corpus: Letras de Zeca Pagodinho

- **Coleta**: Web scraping com BeautifulSoup  
- **Total**: 441 mÃºsicas â†’ 81.348 palavras apÃ³s limpeza

#### ğŸ”§ PrÃ©-processamento

- TokenizaÃ§Ã£o por palavras  
- VocabulÃ¡rio limitado Ã s 5.000 palavras mais frequentes  
- Palavras raras mapeadas para `<unk>`

#### ğŸ—ï¸ Arquitetura do Modelo LSTM

| Componente         | DescriÃ§Ã£o                                                                 |
|--------------------|---------------------------------------------------------------------------|
| Embedding          | Vetores densos de 100 dimensÃµes, aprendidos do zero                       |
| LSTM               | 2 camadas com 256 unidades ocultas cada                                   |
| Camada Linear      | Mapeia saÃ­da da LSTM para distribuiÃ§Ã£o de probabilidade do vocabulÃ¡rio    |

#### âš™ï¸ Treinamento

- **Ã‰pocas**: 15  
- **Otimizador**: Adam (lr = 0.0005)  
- **FunÃ§Ã£o de Perda**: Cross-Entropy Loss

---

### ğŸ“ˆ Resultados e DiscussÃ£o

#### ğŸ”¢ Performance Quantitativa

- **Perplexidade (PPL)**: 654.4384  
- **InterpretaÃ§Ã£o**: Valor alto â†’ dificuldade em prever sequÃªncias nÃ£o vistas â†’ overfitting

#### ğŸ§¾ Performance Qualitativa: GeraÃ§Ã£o de Texto

| Temperatura | CaracterÃ­stica Principal                          | CoerÃªncia                      | RepetiÃ§Ã£o                          |
|-------------|----------------------------------------------------|--------------------------------|------------------------------------|
| 0.7         | Prioriza palavras mais provÃ¡veis                   | Baixa (coerÃªncia de longo prazo) | Alta ("eu nÃ£o vou nÃ£o vou...")     |
| 1.0         | EquilÃ­brio entre coerÃªncia e criatividade          | Fraca (transiÃ§Ãµes abruptas)     | MÃ©dia                              |
| 1.5         | Mais aleatoriedade, suaviza probabilidades         | Muito baixa (saltos temÃ¡ticos)  | Baixa (alta variedade de palavras) |

- VocabulÃ¡rio temÃ¡tico do samba foi mantido

---

### ğŸš§ LimitaÃ§Ãµes do Modelo

- **Corpus Pequeno**: 441 mÃºsicas nÃ£o foram suficientes para generalizaÃ§Ã£o  
- **RepetiÃ§Ã£o e IncoerÃªncia**: Erros predominantes  
- **ViÃ©s TemÃ¡tico**: Palavras como "samba", "amor", "cerveja" dominam  
- **VocabulÃ¡rio Fixo**: Uso de `<unk>` limitou riqueza lexical

---

### âœ… ConclusÃ£o

O modelo LSTM conseguiu capturar o estilo e vocabulÃ¡rio temÃ¡tico de Zeca Pagodinho, mas apresentou:

- **Alta perplexidade**
- **Baixa qualidade textual**
- **Overfitting severo**

ğŸ” A principal liÃ§Ã£o Ã© que **modelos de linguagem treinados do zero exigem grandes volumes de dados** para alcanÃ§ar boa generalizaÃ§Ã£o.

---

Se quiser, posso transformar esse conteÃºdo em uma apresentaÃ§Ã£o ou pÃ´ster acadÃªmico tambÃ©m!
